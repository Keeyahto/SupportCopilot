
import os
import json
from typing import AsyncGenerator, Optional, List

from openai import OpenAI
from sse_starlette.sse import EventSourceResponse

from .deps import state


def system_prompt(mode: str, strict: bool) -> str:
    base = (
        "Ты — лаконичный ассистент поддержки. Отвечай кратко и по делу. "
        "Если используешь материалы из контекста, ставь ссылки на источники в виде [d0], [d1] рядом с фактами. "
        "Не раскрывай внутренние данные."
    )
    if mode == "policies" or strict:
        base += " Соблюдай строгий тон и не давай обещаний вне правил компании."
    return base


def build_user_prompt(question: str, context_sources: list[dict], tool_block: Optional[dict] = None) -> str:
    ctx = []
    for s in context_sources[:5]:
        sid = s.get('id', '')
        ctx.append(f"[{sid}] {s.get('filename','')}: {s.get('snippet','')}")
    ctx_block = "\n".join(ctx)
    tb = ""
    if tool_block:
        tb = f"\n\nИнструменты: {json.dumps(tool_block, ensure_ascii=False)}"
    return f"Вопрос: {question}\n\nКонтекст:\n---\n{ctx_block}\n---{tb}"


def _llm_enabled() -> bool:
    return bool(state.client) and bool(os.getenv("OPENAI_API_KEY")) and bool(os.getenv("CHAT_MODEL"))


def llm_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None) -> str:
    if not _llm_enabled():
        return "Режим офлайн: LLM недоступен. Проверьте настройки OpenAI."
    client: OpenAI = state.client
    resp = client.chat.completions.create(
        model=os.getenv("CHAT_MODEL"),
        messages=[
            {"role": "system", "content": system_prompt("faq", False)},
            {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content or ""


async def llm_stream_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None, labels: Optional[List[str]] = None) -> AsyncGenerator[dict, None]:
    client: OpenAI = state.client
    # send context first
    context_event = {"event": "context", "data": json.dumps({
        "sources": context_sources,
        "labels": labels or [],
        "tool_info": tool_info or {},
    }, ensure_ascii=False)}
    yield context_event

    if not _llm_enabled():
        # Fallback: emit a short stub answer token-by-token
        stub = "Режим офлайн: LLM недоступен."
        for ch in stub:
            token_event = {"event": "token", "data": json.dumps({"t": ch}, ensure_ascii=False)}
            yield token_event
        done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
        yield done_event
        return

    try:
        with client.chat.completions.stream(
            model=os.getenv("CHAT_MODEL"),
            messages=[
                {"role": "system", "content": system_prompt("faq", False)},
                {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
            ],
            temperature=0.2,
        ) as stream:
            for chunk in stream:\n            delta = None\n            last_delta = None
                try:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        delta = chunk.choices[0].delta.content
                    elif hasattr(chunk, 'content'):
                        delta = chunk.content
                    elif hasattr(chunk, 'delta') and hasattr(chunk.delta, 'content'):
                        delta = chunk.delta.content
                    elif hasattr(chunk, 'type') and chunk.type == 'content.delta':
                        delta = getattr(chunk, 'delta', None)
                    elif hasattr(chunk, 'type') and chunk.type == 'chunk':
                        if hasattr(chunk, 'chunk') and hasattr(chunk.chunk, 'choices'):
                            delta = chunk.chunk.choices[0].delta.content
                except Exception:
                    delta = None

                if delta:
                    token_event = {"event": "token", "data": json.dumps({"t": delta}, ensure_ascii=False)}
                    yield token_event
            done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
            yield done_event
