import os
import json
from typing import AsyncGenerator, Optional

from openai import OpenAI
from sse_starlette.sse import EventSourceResponse

from .deps import state


def system_prompt(mode: str, strict: bool) -> str:
    base = (
        "Ð¢Ñ‹ â€” Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾, Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾, Ñ Ñ†Ð¸Ñ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð². "
        "Ð•ÑÐ»Ð¸ Ð´Ð°ÑŽÑ‚ÑÑ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°, Ð½Ðµ Ð¾Ð±ÐµÑ‰Ð°Ð¹ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚Ñ‹ Ð²Ð½Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»."
    )
    if mode == "policies" or strict:
        base += " ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°. Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ â€” Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑÐºÐ°Ð»Ð°Ñ†Ð¸ÑŽ."
    return base


def build_user_prompt(question: str, context_sources: list[dict], tool_block: Optional[dict] = None) -> str:
    ctx = []
    for s in context_sources[:5]:
        ctx.append(f"[{s['filename']}] {s['snippet']}")
    ctx_block = "\n".join(ctx)
    tb = ""
    if tool_block:
        tb = f"\n\nÐ˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚: {json.dumps(tool_block, ensure_ascii=False)}"
    return f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\n\nÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n---\n{ctx_block}\n---{tb}"


def _llm_enabled() -> bool:
    return bool(state.client) and bool(os.getenv("OPENAI_API_KEY")) and bool(os.getenv("CHAT_MODEL"))


def llm_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None) -> str:
    if not _llm_enabled():
        return "Ð”ÐµÐ¼Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚ (LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½). Ð­Ñ‚Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð±ÐµÐ· Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº OpenAI."
    client: OpenAI = state.client
    resp = client.chat.completions.create(
        model=os.getenv("CHAT_MODEL"),
        messages=[
            {"role": "system", "content": system_prompt("faq", False)},
            {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
        ],
        temperature=0.2,
    )
    return resp.choices[0].message.content or ""


async def llm_stream_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None) -> AsyncGenerator[dict, None]:
    print(f"ðŸš€ [LLM] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¸Ð¼ Ð´Ð»Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°: {question[:50]}...")
    print(f"ðŸš€ [LLM] Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸: {len(context_sources)}")
    print(f"ðŸš€ [LLM] Tool info: {tool_info}")
    
    client: OpenAI = state.client
    # send context first
    context_event = {"event": "context", "data": json.dumps({
        "sources": context_sources,
        "labels": [],
        "tool_info": tool_info or {},
    }, ensure_ascii=False)}
    print(f"ðŸ“‹ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ context event")
    yield context_event

    if not _llm_enabled():
        print("âš ï¸ [LLM] LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ fallback")
        # Fallback: emit a short stub answer token-by-token
        stub = "Ð”ÐµÐ¼Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚ (LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½)."
        for ch in stub:
            token_event = {"event": "token", "data": json.dumps({"t": ch}, ensure_ascii=False)}
            print(f"ðŸ”¤ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ fallback Ñ‚Ð¾ÐºÐµÐ½: '{ch}'")
            yield token_event
        done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
        print(f"âœ… [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ done event")
        yield done_event
        return

    print(f"ðŸ¤– [LLM] LLM Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½, Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ OpenAI API")
    try:
        with client.chat.completions.stream(
            model=os.getenv("CHAT_MODEL"),
            messages=[
                {"role": "system", "content": system_prompt("faq", False)},
                {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
            ],
            temperature=0.2,
        ) as stream:
            print(f"ðŸ“¡ [LLM] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¸Ð¼ Ð¾Ñ‚ OpenAI")
            token_count = 0
            last_delta = ""  # Ð”Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            
            for chunk in stream:
                delta = None
                try:
                    # LM Studio Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð´Ñ€ÑƒÐ³ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ñ‡Ð°Ð½ÐºÐ¾Ð²
                    if hasattr(chunk, 'choices') and chunk.choices:
                        # OpenAI Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                        delta = chunk.choices[0].delta.content
                    elif hasattr(chunk, 'content'):
                        # LM Studio Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                        delta = chunk.content
                    elif hasattr(chunk, 'delta') and hasattr(chunk.delta, 'content'):
                        # ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                        delta = chunk.delta.content
                    elif hasattr(chunk, 'type') and chunk.type == 'content.delta':
                        # ContentDeltaEvent Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                        delta = getattr(chunk, 'delta', None)
                    elif hasattr(chunk, 'type') and chunk.type == 'chunk':
                        # ChunkEvent Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
                        if hasattr(chunk, 'chunk') and hasattr(chunk.chunk, 'choices'):
                            delta = chunk.chunk.choices[0].delta.content
                    else:
                        print(f"ðŸ”¤ [LLM] ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ñ‡Ð°Ð½ÐºÐ°: {type(chunk)} - {chunk}")
                        delta = None
                except Exception as e:
                    print(f"âš ï¸ [LLM] ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ delta: {e}")
                    delta = None
                
                if delta and delta != last_delta:  # Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ
                    token_event = {"event": "token", "data": json.dumps({"t": delta}, ensure_ascii=False)}
                    print(f"ðŸ”¤ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½ {token_count}: '{delta}'")
                    yield token_event
                    token_count += 1
                    last_delta = delta
                elif delta:
                    print(f"ðŸ”¤ [LLM] ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½: '{delta}'")
                else:
                    print(f"ðŸ”¤ [LLM] ÐŸÑƒÑÑ‚Ð¾Ð¹ delta Ð² Ñ‡Ð°Ð½ÐºÐµ Ñ‚Ð¸Ð¿Ð°: {type(chunk)}")
            
            print(f"âœ… [LLM] Ð¡Ñ‚Ñ€Ð¸Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {token_count}")
            done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
            print(f"âœ… [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ done event")
            yield done_event
    except Exception as e:
        print(f"ðŸ’¥ [LLM] ÐžÑˆÐ¸Ð±ÐºÐ° Ð² ÑÑ‚Ñ€Ð¸Ð¼Ðµ: {e}")
        error_event = {"event": "error", "data": json.dumps({"message": str(e)})}
        print(f"âŒ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ error event")
        yield error_event


def sse_from_generator(gen: AsyncGenerator[dict, None]) -> EventSourceResponse:
    async def event_gen():
        try:
            async for ev in gen:
                yield ev
        except Exception as e:
            yield {"event": "error", "data": json.dumps({"message": str(e)})}
    return EventSourceResponse(event_gen())
