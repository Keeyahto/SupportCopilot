   1: import os
   2: import json
   3: from typing import AsyncGenerator, Optional, List
   4: 
   5: from openai import OpenAI
   6: from sse_starlette.sse import EventSourceResponse
   7: 
   8: from .deps import state
   9: 
  10: 
  11: def system_prompt(mode: str, strict: bool) -> str:
  12:     base = (
  13:         "Ð¢Ñ‹ â€” Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾, Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ð¾, Ñ Ñ†Ð¸Ñ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð². "
  14:         "Ð•ÑÐ»Ð¸ Ð´Ð°ÑŽÑ‚ÑÑ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°, Ð½Ðµ Ð¾Ð±ÐµÑ‰Ð°Ð¹ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚Ñ‹ Ð²Ð½Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»."
  15:     )
  16:     if mode == "policies" or strict:
  17:         base += " ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°. Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ â€” Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑÑÐºÐ°Ð»Ð°Ñ†Ð¸ÑŽ."
  18:     return base
  19: 
  20: 
  21: def build_user_prompt(question: str, context_sources: list[dict], tool_block: Optional[dict] = None) -> str:
  22:     ctx = []
  23:     for s in context_sources[:5]:
  24:         ctx.append(f"[{s['filename']}] {s['snippet']}")
  25:     ctx_block = "\n".join(ctx)
  26:     tb = ""
  27:     if tool_block:
  28:         tb = f"\n\nÐ˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚: {json.dumps(tool_block, ensure_ascii=False)}"
  29:     return f"Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\n\nÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n---\n{ctx_block}\n---{tb}"
  30: 
  31: 
  32: def _llm_enabled() -> bool:
  33:     return bool(state.client) and bool(os.getenv("OPENAI_API_KEY")) and bool(os.getenv("CHAT_MODEL"))
  34: 
  35: 
  36: def llm_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None) -> str:
  37:     if not _llm_enabled():
  38:         return "Ð”ÐµÐ¼Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚ (LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½). Ð­Ñ‚Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼ Ð±ÐµÐ· Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº OpenAI."
  39:     client: OpenAI = state.client
  40:     resp = client.chat.completions.create(
  41:         model=os.getenv("CHAT_MODEL"),
  42:         messages=[
  43:             {"role": "system", "content": system_prompt("faq", False)},
  44:             {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
  45:         ],
  46:         temperature=0.2,
  47:     )
  48:     return resp.choices[0].message.content or ""
  49: 
  50: 
  51: async def llm_stream_answer(question: str, context_sources: list[dict], tool_info: Optional[dict] = None, labels: Optional[List[str]] = None) -> AsyncGenerator[dict, None]:
  52:     print(f"ðŸš€ [LLM] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¸Ð¼ Ð´Ð»Ñ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°: {question[:50]}...")
  53:     print(f"ðŸš€ [LLM] Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸: {len(context_sources)}")
  54:     print(f"ðŸš€ [LLM] Tool info: {tool_info}")
  55:     
  56:     client: OpenAI = state.client
  57:     # send context first
  58:     context_event = {"event": "context", "data": json.dumps({
  59:         "sources": context_sources,
  60:         "labels": labels or [],
  61:         "tool_info": tool_info or {},
  62:     }, ensure_ascii=False)}
  63:     print(f"ðŸ“‹ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ context event")
  64:     yield context_event
  65: 
  66:     if not _llm_enabled():
  67:         print("âš ï¸ [LLM] LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ fallback")
  68:         # Fallback: emit a short stub answer token-by-token
  69:         stub = "Ð”ÐµÐ¼Ð¾-Ð¾Ñ‚Ð²ÐµÑ‚ (LLM Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½)."
  70:         for ch in stub:
  71:             token_event = {"event": "token", "data": json.dumps({"t": ch}, ensure_ascii=False)}
  72:             print(f"ðŸ”¤ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ fallback Ñ‚Ð¾ÐºÐµÐ½: '{ch}'")
  73:             yield token_event
  74:         done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
  75:         print(f"âœ… [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ done event")
  76:         yield done_event
  77:         return
  78: 
  79:     print(f"ðŸ¤– [LLM] LLM Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½, Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ OpenAI API")
  80:     try:
  81:         with client.chat.completions.stream(
  82:             model=os.getenv("CHAT_MODEL"),
  83:             messages=[
  84:                 {"role": "system", "content": system_prompt("faq", False)},
  85:                 {"role": "user", "content": build_user_prompt(question, context_sources, tool_info)},
  86:             ],
  87:             temperature=0.2,
  88:         ) as stream:
  89:             print(f"ðŸ“¡ [LLM] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¸Ð¼ Ð¾Ñ‚ OpenAI")
  90:             token_count = 0
  91:             last_delta = ""  # Ð”Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
  92:             
  93:             for chunk in stream:
  94:                 delta = None
  95:                 try:
  96:                     # LM Studio Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð´Ñ€ÑƒÐ³ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ñ‡Ð°Ð½ÐºÐ¾Ð²
  97:                     if hasattr(chunk, 'choices') and chunk.choices:
  98:                         # OpenAI Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
  99:                         delta = chunk.choices[0].delta.content
 100:                     elif hasattr(chunk, 'content'):
 101:                         # LM Studio Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
 102:                         delta = chunk.content
 103:                     elif hasattr(chunk, 'delta') and hasattr(chunk.delta, 'content'):
 104:                         # ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
 105:                         delta = chunk.delta.content
 106:                     elif hasattr(chunk, 'type') and chunk.type == 'content.delta':
 107:                         # ContentDeltaEvent Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
 108:                         delta = getattr(chunk, 'delta', None)
 109:                     elif hasattr(chunk, 'type') and chunk.type == 'chunk':
 110:                         # ChunkEvent Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚
 111:                         if hasattr(chunk, 'chunk') and hasattr(chunk.chunk, 'choices'):
 112:                             delta = chunk.chunk.choices[0].delta.content
 113:                     else:
 114:                         print(f"ðŸ”¤ [LLM] ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ñ‡Ð°Ð½ÐºÐ°: {type(chunk)} - {chunk}")
 115:                         delta = None
 116:                 except Exception as e:
 117:                     print(f"âš ï¸ [LLM] ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ delta: {e}")
 118:                     delta = None
 119:                 
 120:                 if delta and delta != last_delta:  # Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ
 121:                     token_event = {"event": "token", "data": json.dumps({"t": delta}, ensure_ascii=False)}
 122:                     print(f"ðŸ”¤ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾ÐºÐµÐ½ {token_count}: '{delta}'")
 123:                     yield token_event
 124:                     token_count += 1
 125:                     last_delta = delta
 126:                 elif delta:
 127:                     print(f"ðŸ”¤ [LLM] ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚Ð¾ÐºÐµÐ½: '{delta}'")
 128:                 else:
 129:                     print(f"ðŸ”¤ [LLM] ÐŸÑƒÑÑ‚Ð¾Ð¹ delta Ð² Ñ‡Ð°Ð½ÐºÐµ Ñ‚Ð¸Ð¿Ð°: {type(chunk)}")
 130:             
 131:             print(f"âœ… [LLM] Ð¡Ñ‚Ñ€Ð¸Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {token_count}")
 132:             done_event = {"event": "done", "data": json.dumps({"finish_reason": "stop"}, ensure_ascii=False)}
 133:             print(f"âœ… [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ done event")
 134:             yield done_event
 135:     except Exception as e:
 136:         print(f"ðŸ’¥ [LLM] ÐžÑˆÐ¸Ð±ÐºÐ° Ð² ÑÑ‚Ñ€Ð¸Ð¼Ðµ: {e}")
 137:         error_event = {"event": "error", "data": json.dumps({"message": str(e)})}
 138:         print(f"âŒ [LLM] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ error event")
 139:         yield error_event
 140: 
 141: 
 142: def sse_from_generator(gen: AsyncGenerator[dict, None]) -> EventSourceResponse:
 143:     async def event_gen():
 144:         try:
 145:             async for ev in gen:
 146:                 yield ev
 147:         except Exception as e:
 148:             yield {"event": "error", "data": json.dumps({"message": str(e)})}
 149:     return EventSourceResponse(event_gen())
